Модуль tokenizer может быть использован для токенизации текстов на английском языке тремя разными способами. 

Для токенизации используются следующие библиотеки:

1. re 
Библиотека re позволяет токенизировать текст наиболее простым способом — по пробелом и знакам препинания. Такой метод не подходит для работы с разными языками и морфологией, а также для разрешения сложных и серьезных задач из-за функциональной ограниченности.

2. NLTK
Библиотека NLTK является полезным инструментом для символьной и статистической обработки естественного языка. Она позволяет просто предобрабатывать текст. Однако эта библиотека не подходит для работы с большими объемами данных, потому что она медленно работает на больших корпусах.

3. SpaCy 
Библиотека SpaCy также используется для токенизации естественного языка. SpaCy лучше всего подходит для работы с морфологией и синтаксисом. Однако эта библиотека требует больших ресурсов 

Проект состоит из основного модуля _tokenizer.py_, демонстративного скрипта _demo.py_, файла зависимостей _requirements.txt_ и документации _README.md_.

# Инструкция по установке:

Клонирование репозитория 

  ```git clone https://github.com/bugristaya/comp_ling.git```

Переход в папку с модулем

 ``` cd comp_ling\tokenization_hw```


# Настройка окружения
Создание виртуального окружения 
 ```
  python3 -m venv tokenizer_env
  source tokenizer_env/bin/activate
```

# Установка зависимостей
 ```
  pip install -r requirements.txt
```

# Дополнительно для spaCy (после установки spaCy)
  ```
  python3 -m spacy download en_core_web_sm
```


# Использование модуля:
```
# Импорт модуля
  from tokenizer import TextTokenizer
# Создание токенизатора
  tokenizer = TextTokenizer()
# Использование разных методов токенизации отдельно 
  text = "Hello, world! This is a test sentence."
```

  # Простая токенизация
 ``` tokens = tokenizer.simple_tokenize(text) ```
  # NLTK токенизация 
 ``` nltk_tokens = tokenizer.nltk_tokenize(text) ```
  # SpaCy Токенизация 
``` sp_tokens = tokenizer.spacy_tokenize(text) ```

# Использование всех методов 
 ``` results = tokenizer.tokenize_all(text) ```

# Описание методов 

**simple_tokenize(text)**
Токенизация с использованием регулярных выражений. Разбивает текст на слова по пробелам и знакам препинания.

Параметр — text (str).  
Функция возвращает list (список токенов)
Если необходимая библиотека не установлена, то выводится ошибка импорта

**nltk_tokenize(text)**
Токенизация с использованием библиотеки NLTK.  Знаки препинания — отдельные токены.

Параметр — text (str)
Возвращает list (Список токенов)
Если необходимая библиотека не установлена, то выводится ошибка импорта 

**spacy_tokenize(text)**
Токенизация с использованием библиотеки spaCy. Знаки препинания — отдельные токены. 

Параметры — text (str)
Возвращает list (Список токенов)
Если необходимая библиотека не установлена, то выводится ошибка импорта

# tokenize_all(text)
Применяет все три метода токенизации к тексту.

Параметр — text (str)
Возвращает словарь с результатами всех методов, где method — тип токенизации, а tokens — результат токенизации сопутствующим методом. 


# Демонстрация demo.py
```
# Тестирование модуля
  python3 demo.py


    from tokenizer import TextTokenizer
    
    def main():
        tokenizer = TextTokenizer()
    
        # Пример текста для токенизации
        sample_text = "Hello, world! This is a test sentence. How are you today?"
    
        # Применяем все методы токенизации
        results = tokenizer.tokenize_all(sample_text)
    
        # Выводим результаты
        for method, tokens in results.items():
            print(f"{method}: {tokens}")
    
    if __name__ == "__main__":
        main()
```

Вывод следующий:
```
  simple: ['Hello', 'world', 'This', 'is', 'a', 'test', 'sentence', 'How',  'are', 'you', 'today']
  NLTK: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.', 'How', 'are', 'you', 'today', '?']
  spaCy: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.', 'How', 'are', 'you', 'today', '?']
```

